import type { NextApiRequest, NextApiResponse } from 'next';
import OpenAI from 'openai';
import { LlmPrompts } from '@/lib/common';

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  console.log('ðŸŸ¦ Received request:', req.method);

  if (req.method !== 'POST') {
    console.log('ðŸ”´ Invalid method:', req.method);
    return res.status(405).json({ error: 'Method not allowed' });
  }

  if (!process.env.OPENAI_API_URL || !process.env.OPENAI_MODEL) {
    console.log('ðŸ”´ Missing OpenAI config');
    return res.status(500).json({ error: 'OpenAI configuration is missing' });
  }

  try {
    const { message, purpose } = req.body;
    console.log('ðŸŸ© Processing message:', message);

    if (!message || !purpose) {
      console.log('ðŸ”´ Missing message or purpose');
      return res.status(400).json({ error: 'Message and purpose are required' });
    }

    let prompt = LlmPrompts[purpose] + message;

    console.log('ðŸŸ¨ Creating OpenAI client with URL:', process.env.OPENAI_API_URL);
    const client = new OpenAI({
      baseURL: process.env.OPENAI_API_URL
    });

    console.log('ðŸŸª Requesting chat completion with model:', process.env.OPENAI_MODEL);
    const chatCompletion = await client.chat.completions.create({
      messages: [{ role: 'user', content: prompt }],
      model: process.env.OPENAI_MODEL
    });

    console.log('âœ… Chat completion successful');
    return res.status(200).json(chatCompletion);

  } catch (error) {
    console.log('ðŸ”´ Error occurred:', error);
    console.error('Chat completion error:', error);
    return res.status(500).json({ error: 'Failed to get chat completion' });
  }
}
